# Evaluation (refereeing)



{% hint style="info" %}
We refer to 'evaluation' because the Unjournal does not _publish_ work; it only links, rates and evaluates it. However, below, we will mainly refer to traditional terms like 'referees', indicating the same thing. We do this to make it more readable and familiar.
{% endhint %}

## Choosing, working with evaluators (referees)

### How do we choose 'referees'?

### Why pay referees?



## Public evaluations

### Why public evaluations?

#### Exceptions for ECRs?

### How are these hosted and shared?



## Evaluation guidelines and criteria

### General criteria/guidelines

### Global priorities emphasis/discussion

### Open science criteria/emphasis

### Quantitative ratings/metrics

* Single and multiple dimension
* Predict or evaluate 'traditional publication outcome' as a benchmark&#x20;

## Anonymity/blinding vs. signed reports

### Feedback and discussion vs. evaluations

DR: I suspect that signed reviews (cf blog posts) provide good feedback and evaluation. However, when it comes to rating (quantitative measures of a paper's value), my impression from existing initiatives and conversations is that people are reluctant to award anything less than 5/5 'full  marks'.

### Why Single-blind?

* Power dynamics: referees don't want to be 'punished', may want to flatter powerful authors
* Connections and friendships may inhibit honesty
* 'Powerful referees signing critical reports' could hurt ECRs&#x20;

### Why signed reports?

* Public reputation incentive for referees
  * (But note single-blind paid review has some private incentives.)&#x20;
* Fosters better public dialogue
* Inhibits obviously unfair and impolite 'trashing'

### Compromise approaches

* Author and referee choose
* Random trial: We can compare empirically (are signed reviews less informative?)
* Use a mix (1 signed, 2 anonymous reviews) for each paper



## &#x20;
